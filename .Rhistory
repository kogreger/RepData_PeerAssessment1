GTD.Export <- read.csv("~/Downloads/GTD-Export.csv")
View(GTD.Export)
min(GTD.Export$DATE)
gtd <- GTD.Export
view(gtd)
View(gtd)
View(GTD.Export)
range(gtd$DATE)
range(gtd$DATE, na.rm=true)
range(gtd$DATE, na.rm=T)
describe.by(gtd, gtd$CITY)
library(psych)
describe.by(gtd, gtd$CITY)
# load necessary libraries
library(ggplot2)
library(scales)
library(reshape)
# load and massage data
tripchain <- read.csv("~/Documents/University of Tsukuba/PhD Thesis/data/trip_chain_matrix.csv", sep=";")
tripchain.m <- melt(tripchain)
tripchain.m$from <- factor(tripchain.m$from, levels=tripchain$from)
tripchain.m[order(tripchain.m$from, decreasing = T),]
# create and display heatmap
(p <- ggplot(tripchain.m, aes(variable, from)) + geom_tile(aes(fill = value), colour = "white") + scale_fill_gradient(low = "blue", high = "red"))
base_size <- 12
p <- p + theme_bw(base_size = base_size) + labs(x = "to", y = "from") + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + opts(legend.position = "none", axis.ticks = theme_blank(), axis.text.x = theme_text(size = base_size, angle = 90, hjust = 1, color = "black"), axis.text.y = theme_text(size = base_size, hjust = 0, color = "black"))
p <- p + opts(axis.title.x = theme_text(face="bold", size=12, angle=0, hjust=0.5))
p <- p + opts(axis.title.y = theme_text(face="bold", size=12, angle=90, vjust=0.5))
print(p)
ggsave(p, file = "tripchain_heatmap.png", path = "~/Documents/University of Tsukuba/PhD Thesis/img/", width = 166, height = 80, units = "mm", dpi = 300)
ggsave(p, file = "tripchain_heatmap.png", path = "~/Documents/University of Tsukuba/PhD Thesis/img/", width = 166, height = 100, units = "mm", dpi = 300)
tripchain <- read.csv("~/Documents/University of Tsukuba/PhD Thesis/data/trip_chain_matrix.csv", sep=";")
tripchain.m <- melt(tripchain)
tripchain.m$from <- factor(tripchain.m$from, levels=tripchain$from)
tripchain.m[order(tripchain.m$from, decreasing = T),]
# create and display heatmap
(p <- ggplot(tripchain.m, aes(variable, from)) + geom_tile(aes(fill = value), colour = "white") + scale_fill_gradient(low = "blue", high = "red"))
base_size <- 12
p <- p + theme_bw(base_size = base_size) + labs(x = "to", y = "from") + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + opts(legend.position = "none", axis.ticks = theme_blank(), axis.text.x = theme_text(size = base_size, angle = 90, hjust = 1, color = "black"), axis.text.y = theme_text(size = base_size, hjust = 0, color = "black"))
p <- p + opts(axis.title.x = theme_text(face="bold", size=12, angle=0, hjust=0.5))
p <- p + opts(axis.title.y = theme_text(face="bold", size=12, angle=90, vjust=0.5))
print(p)
ggsave(p, file = "tripchain_heatmap.png", path = "~/Documents/University of Tsukuba/PhD Thesis/img/", width = 166, height = 100, units = "mm", dpi = 300)
dat = read.csv("/Volumes/WD_8TB/Data_Transfer/murayama.csv", header = TRUE)
dat = read.csv("/Volume/WD_8TB/Data_Transfer/murayama.csv", header = TRUE)
dat = read.csv("/Volume/WD_8TB/Data_Transfer/murayama.txt", header = TRUE)
dat = read.csv("/Volumes/WD_8TB/Data_Transfer/murayama.txt", header = TRUE)
dat
head(dat)
dcast(dat, age ~ work)
require(reshape2)
dcast(dat, age ~ work)
require(reshape2)
dat = read.csv("/Volumes/WD_8TB/Data_Transfer/pflow_age_work.txt", header = TRUE)
dcast(dat, age ~ work)
View(dat)
table = dcast(dat, age ~ work)
View(table)
write.csv(table, "/Volumes/WD_8TB/Data_Transfer/pflow_age_work_table.csv")
install.packages("ggplot2")
install.packages(c("class", "digest", "foreign", "lattice", "MASS", "Matrix", "mgcv", "nlme", "nnet", "psych", "rgl", "rpart", "sm", "spatial", "survival"))
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/raillink_usage.csv",head=TRUE,sep=",")
View(df)
dcast(df, raillink ~ phour)
require(reshape2)
dcast(df, raillink ~ phour)
newdf <- dcast(df, raillink ~ phour)
View(newdf)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/raillink_usage2.csv", sep=",")
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/raillink_usage2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/raillink_usage.csv", head=TRUE, sep=",")
newdf <- dcast(df, raillink ~ phour)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/raillink_usage2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/raillink_usage22.csv", head=TRUE, sep=",")
newdf <- dcast(df, raillink ~ phour)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/raillink_usage222.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/raillink_usage.csv", head=TRUE, sep=",")
newdf <- dcast(df, raillink ~ phour)
View(newdf)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/raillink_usage2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_sbp.txt", head=TRUE, sep=";")
View(df)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_sbp.txt", head=TRUE, sep=";")
View(df)
newdf <- dcast(df, level ~ hour)
View(newdf)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/vuln_sbp2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_inst.txt", head=TRUE, sep=";")
View(df)
newdf <- dcast(df, type + level ~ hour)
View(newdf)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/vuln_inst2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_tsu.txt", head=TRUE, sep=";")
newdf <- dcast(df, level ~ hour)
View(newdf)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/vuln_tsu2.csv", sep=",", row.names=FALSE)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_rli.txt", head=TRUE, sep=";")
View(df)
newdf <- dcast(df, level ~ hour)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/vuln_rli2.csv", sep=",", row.names=FALSE)
require(reshape2)
df <- read.csv(file="/Volumes/WD_8TB/Data_Transfer/vuln_btw.txt", head=TRUE, sep=";")
newdf <- dcast(df, level ~ hour)
write.table(newdf, "/Volumes/WD_8TB/Data_Transfer/vuln_btw2.csv", sep=",", row.names=FALSE)
View(newdf)
library(ggplot2)
data(diamonds)
summary
summary(diamonds)
str(diamonds)
levels(diamonds$clarity)
levels(diamonds$color)
diamonds$color
?diamonds
qplot(data = diamonds, x = price)
summary(diamonds)
summary(diamonds$price)
subset(diamonds, price < 5000)
lt5000 <- subset(diamonds, price < 5000)
lt500 <- subset(diamonds, price < 500)
lt250 <- subset(diamonds, price < 250)
gt15000 <- subset(diamonds, price >= 15000)
qplot(data = diamonds, x = price)
qplot(data = diamonds, x = price, xlim = c(0,2500))
qplot(data = diamonds, x = price, xlim = c(500,1000))
qplot(data = diamonds, x = price, xlim = c(500,1000), binwidth = 1)
qplot(data = diamonds, x = price, xlim = c(0,2000), binwidth = 1)
qplot(data = diamonds, x = price) +
facet_wrap(~ color)
qplot(data = diamonds, x = price) +
facet_wrap(~ cut)
by(diamonds, cut, summary)
by(diamonds, cut, str)
?by
str(diamonds)
by(diamonds, diamonds$cut, str)
by(diamonds, diamonds$cut, summary)
by(diamonds$price, diamonds$cut, summary)
by(diamonds$price, diamonds$cut, max)
qplot(x = price, data = diamonds) + facet_wrap(~cut)
?facet_wrap
qplot(x = price, data = diamonds) + facet_wrap(~cut, scales = free)
qplot(x = price, data = diamonds) + facet_wrap(~cut, scales = "free"")
qplot(x = price, data = diamonds) + facet_wrap(~cut, scales = "free")
qplot(x = price, y = carat, data = diamonds) + facet_wrap(~cut, scales = "free")
qplot(x = price / carat, data = diamonds) + facet_wrap(~cut, scales = "free")
qplot(x = price / carat, data = diamonds) + facet_wrap(~cut, scales = "free") + scale_x_log10()
?qplot
qplot(data = diamonds, x = price, geom = "boxplot") + facet_wrap(~ cut)
qplot(data = diamonds, x = cut, y = price, geom = "boxplot")
qplot(data = diamonds, x = clarity, y = price, geom = "boxplot")
qplot(data = diamonds, x = color, y = price, geom = "boxplot")
str(subset(diamonds, color == D))
str(subset(diamonds, color = D))
str(subset(diamonds$price, color = D))
str(subset(diamonds, color = D))
summary(subset(diamonds, color = D))
summary(subset(diamonds$price, color = D))
summary(subset(diamonds, color = D))
summary(subset(diamonds, color = D)$price)
IQR(subset(diamonds, color = D)$price)
summary(subset(diamonds, color = J)$price)
IQR(subset(diamonds, color = J)$price)
?diamonds
summary(subset(diamonds, color == J)$price)
?IQR
by(diamonds$price, diamonds$color, summary)
by(diamonds$price, diamonds$color, IQR)
qplot(data = diamonds, x = price, y = carat) + facet_wrap(~ color)
qplot(carat,data=diamonds,geom="freqpoly",binwidth=0.01,color=I('black'))+scale_x_continuous(limit=c(0,5),breaks=seq(0,5,0.1))+scale_y_continuous(breaks=seq(0,10000,1000))
dhk09_0000 <- read.table("/Volumes/WD_8TB/Data_Transfer/pflow/dhk09/09dhk_0000.csv", sep = ",", header = FALSE)
head(dhk09_0000)
object.size(dhk09_0000)
mnl96_0000 <- read.table("/Volumes/WD_8TB/Data_Transfer/pflow/mnl96/mnl96_0000.csv", sep = ",", header = FALSE)
library(datasets)
data(iris)
?iris
names(iris)
levels(iris$Species)
by(iris$Sepal.Length, iris$Species, mean)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
?mtcars
tapply(mtcars$mpg, mtcars$cyl, mean)
names(mtcars)
mean(mtcars[cyl == 4, "hp"])
mean(mtcars["cyl" == 4, "hp"])
mean(mtcars$hp["cyl" == 4, ])
mean(mtcars$hp["cyl" == 4])
mean(mtcars$hp["cyl" == 4], na.rm = T)
tapply(mtcars$hp, mtcars$cyl, mean)
mtcars$hp["cyl" == 4]
mtcars$hp["cyl" = 4]
mtcars["cyl" = 4]$hp
mtcars["cyl" = 4, "hp"]
mtcars[cyl = 4]
mtcars[cyl = 4, ]
mtcars[mtcars$cyl = 4, ]
mtcars[mtcars$cyl == 4, ]
mean(mtcars[mtcars$cyl == 4, ])
mean(mtcars[mtcars$cyl == 4, ]$hp)
mean(mtcars[mtcars$cyl == 4, ]$hp) - mean(mtcars[mtcars$cyl == 8, ]$hp)
abs(mean(mtcars[mtcars$cyl == 4, ]$hp) - mean(mtcars[mtcars$cyl == 8, ]$hp))
debug(ls)
ls
?ls
ls(mtcars)
debug(ls)
ls(mtcars)
\q
q
q
exit
exit()
debug(ls)
ls(mtcars)
;
library(datasets)
data(mtcars)
debug(ls)
ls(mtcars)
set.seed(20)
x <- rnorm(100)
e <- rnorm(100, 0, 2)
Y <- 0.5 + 2 * x + e
summary(y)
summary(Y)
plot(x, Y)
set.seed(10)
x <- rbinom(100, 1, 0.5)
Y <- 0.5 + 2 * x + e
summary(Y)
plot(x, Y)
library(swirl)
install.packages("swirl")
library(swirl)
swirl()
x <- c(44, NA, 5, NA)
x * 3
y <- rnorm(1000)
z <- rep(NA, 1000)
my_data <- sample(c(y, z), 100)
my_na <- is.na(my_data)
my_na
my_data == NA
sum(my_na)
my_data
0 / 0
Inf - Inf
x
x[1:10]
x[is.na(x)]
x[!is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[c(3, 5, 7)]
x[0]
x[3000]
x[c(-2, -10)]
x[-c(2, 10)]
c(foo = 11, bar = 2, norf = NA)
vect <- c(foo = 11, bar = 2, norf = NA)
vect
nmae(vect)
names(vect)
vect2 <- c(11, 2, NA)
names(vect2) <- c("foo", "bar", "norf")
identical(vect, vect2)
vect["bar"]
vect[c("foo", "bar")]
my_vector <- c(1:20)
my_vector <- 1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector) <- c(4, 5)
dim(my_vector)
attributes(my_vector)
my_vector
class(my_vector)
my_matrix <- my_vector
?matrix
my_matrix2 <- matrix(1:20, nrow = 5, ncol = 5)
my_matrix2 <- matrix(data = 1:20, nrow = 5, ncol = 5)
my_matrix2 <- matrix(data = 1:20, nrow = 4, ncol = 5)
identical(my_matrix, my_matrix2)
patients < -
;
patients <- c("Bill", "Gina", "Kelly", "Sean")
cbind(patients, my_matrix)
my_data <- data.frame(patients, my_matrix)
my_data
class(my_data)
cnames <- c("patient", "age", "weight", "bp", "rating", "test")
colnames(my_data, cnames)
?colnaames
?colnames
colnames(my_data) <- cnames
my_data
# load necessary libraries
library(lattice)
library(plyr)
library(reshape)
# set working directory
setwd("~/Documents/Coursera/Data Science Specialization/Reproducible Research/RepData_PeerAssessment1")
# read data directly from zip archive
data <- read.csv(unz("activity.zip", "activity.csv"),
header = TRUE, quote = "\"", sep = ",", na.strings = "NA",
colClasses = c("numeric", "character", "numeric"))
# convert date column to date data type
data$date <- as.Date(data$date, "%Y-%m-%d")
# calculate total number of steps per day
sum_data <- ddply(data, c("date"), summarize,
tot_steps = sum(steps, na.rm = TRUE))
# calculate median and mean number of steps per day
mean_steps <- mean(sum_data$tot_steps, na.rm = TRUE)
median_steps <- median(sum_data$tot_steps, na.rm = TRUE)
# draw histogram of total number of steps per day
hist(sum_data$tot_steps, breaks = 50,
col = "blue",
main = "Number of Steps Taken per Day",
xlab = "Steps Taken",
ylab = "Count")
# calculate average number of steps per interval
interval_data <- ddply(data, c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
# draw time series plot
with (interval_data, plot(x = interval,
y = mean_steps,
type = "l",
main = "Time Series of Average Steps Taken\nper 5-minute Time Interval",
xlab = "5-minute Time Interval",
ylab = "Average Steps Taken"))
# identify most active time interval
interval_data_sort <- interval_data[order(-interval_data$mean_steps), ]
# calculate number of rows with missing values
missing_data <- sum(is.na(data$steps))
# impute missing data as mean number of steps for respective time interval
data_impute <- merge(data, interval_data, by = "interval")
data_impute[is.na(data_impute$steps), "steps"] <-
data_impute[is.na(data_impute$steps), "mean_steps"]
# calculate total number of steps per day for imputed data
sum_data_impute <- ddply(data_impute, c("date"), summarize,
tot_steps = sum(steps, na.rm = TRUE))
# calculate median and mean number of steps per day
mean_steps_impute <- mean(sum_data_impute$tot_steps, na.rm = TRUE)
median_steps_impute <- median(sum_data_impute$tot_steps, na.rm = TRUE)
# draw histogram of total number of steps per day for imputed data
hist(sum_data_impute$tot_steps, breaks = 50,
col = "blue",
main = "Number of Steps Taken per Day (Imputed Data)",
xlab = "Steps Taken",
ylab = "Count")
data_impute$weekend <- weekdays(data_impute$date) %in% c("Saturday", "Sunday")
interval_data_weekend <- ddply(data_impute[data_impute$weekday == TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
interval_data_weekdays <- ddply(data_impute[data_impute$weekday != TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
interval_data2 <- merge(interval_data_weekend, interval_data_weekdays,
by = "interval",
suffixes = c("_weekend", "_weekdays"))
colnames(interval_data2) <- c("interval", "Weekend", "Weekdays")
interval_data2 <- melt(interval_data2, id = "interval")
interval_data2
interval_data_weekend
interval_data_weekdays
data_impute$weekend
ddply(data_impute[data_impute$weekday == TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
weekdays(data_impute$date) %in% c("Saturday", "Sunday")
head(data_impute)
ddply(data_impute[data_impute$weekend == TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
ddply(data_impute[data_impute$weekend != TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
interval_data_weekend <- ddply(data_impute[data_impute$weekend == TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
interval_data_weekdays <- ddply(data_impute[data_impute$weekend != TRUE, ],
c("interval"), summarize,
mean_steps = mean(steps, na.rm = TRUE))
interval_data2 <- merge(interval_data_weekend, interval_data_weekdays,
by = "interval")
colnames(interval_data2) <- c("interval", "Weekend", "Weekdays")
interval_data2 <- melt(interval_data2, id = "interval")
xyplot(value ~ interval | variable, data = interval_data2,
type="l",
main="Comparison of Average Steps Taken per 5-minute Interval\non Weekdays and Weekends",
ylab="Average Steps Taken",
xlab="5-minute Time Interval")
